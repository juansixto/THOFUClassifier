package DemoClassify;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.util.Iterator;
import java.util.List;
import java.util.Properties;

import edu.stanford.nlp.classify.RVFDataset;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.RVFDatum;
import edu.stanford.nlp.ling.CoreAnnotations.LemmaAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;

import edu.stanford.nlp.stats.ClassicCounter;
import edu.stanford.nlp.util.CoreMap;
import Corpus.Document;
import Corpus.Corpus;
import Qwordnet.QWordNetDB;

public class CorpusTestGenerator {
	
	private final static String[] NEGATION_TOKENS = {"not", "nt", "neither", "nor"};
	static int TestSplit = 20; //%
	int Num_Test = 0;
	int Num_Train = 0;
	
	public String Generate(Corpus corpus) throws IOException
	{
		
		int CorpusMax= (corpus.size())*TestSplit/100;
		QWordNetDB qwordnet = QWordNetDB.createInstance();
		int Num_Test = 0;
		int Num_Train = 0;
		Properties props = new Properties();
		props.put("annotators", "tokenize, ssplit, pos, lemma");
		String  tFichero = "data/THOFUDemo.test";
		String  trFichero = "data/THOFUDemo.train";
		File  TrainFile = new File (trFichero);
		File  TestFile = new File (tFichero);
		BufferedWriter  trainf = new BufferedWriter (new FileWriter (TrainFile));
		BufferedWriter  testf = new BufferedWriter (new FileWriter (TestFile));
		
		StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
		RVFDataset<String, String> dataset = new RVFDataset<String, String>();
		int nDoc = 1;
		for (Document doc : corpus) {
			
			final Annotation document = new Annotation(doc.getText());
			pipeline.annotate(document);

			final List<CoreMap> sentences = document.get(SentencesAnnotation.class);

			ClassicCounter<String> featureCounter = new ClassicCounter<String>();
			for (int h = 0; h < sentences.size(); h++) {
				final CoreMap sentence = sentences.get(h);
				final List<CoreLabel> tokens = sentence.get(TokensAnnotation.class);
				final Iterator<CoreLabel> it = tokens.iterator();
				
				while(it.hasNext()) {
					final String pos = it.next().get(PartOfSpeechAnnotation.class);
					if(pos.equals(".") || pos.equals(",") || pos.equals("``")) {
						it.remove();
					}
				}
				
				boolean negate = false;
				
				for (int i = 0; i < tokens.size(); i++) {
					
					
					
					final CoreLabel token = tokens.get(i);
					String lemma = token.get(LemmaAnnotation.class).toLowerCase();
					String word = token.get(TextAnnotation.class).toLowerCase();
					String wordPos = token.get(PartOfSpeechAnnotation.class);
				
					//String lemma = getLemma(lemmatizer, word, wordPos);
					
					if(wordPos.startsWith("NN") || wordPos.startsWith("JJ") || wordPos.startsWith("RB")) {
						featureCounter.incrementCount("lemma_" + lemma);
						featureCounter.incrementCount("lemma_" + lemma + "_POS_" + wordPos);
						
						if(negate) {
							featureCounter.incrementCount("not_lemma_" + lemma);
						}
					}
					
					if(wordPos.startsWith("NN")) {
						featureCounter.incrementCount("lemma_" + lemma + "_lemma-1_" + ((i == 0)?"<s>":tokens.get(i-1).get(LemmaAnnotation.class).toLowerCase()));
						featureCounter.incrementCount("lemma_" + lemma + "_lemma+1_" + ((i == tokens.size() - 1)?"</s>":tokens.get(i+1).get(LemmaAnnotation.class).toLowerCase()));
						for(int j = i - 1; j >= 0; j--) {
							if(tokens.get(j).get(PartOfSpeechAnnotation.class).startsWith("JJ")) {
								String prevLemma = tokens.get(j).get(LemmaAnnotation.class).toLowerCase();
								String prevLemmaPos = tokens.get(j).get(PartOfSpeechAnnotation.class);
								featureCounter.incrementCount("lemma_" + lemma + "_jj_" + prevLemma);
								
								final String polarityFeature = "lemma_" + lemma + "_polarity";
							final boolean negative = negate != ((qwordnet.getPolarity(prevLemma, prevLemmaPos) * qwordnet.getPolarity(lemma, wordPos)) < 0);
								if(negative) {
									featureCounter.decrementCount(polarityFeature);
								} else {
									featureCounter.incrementCount(polarityFeature);
								}
								break;
							}
						}
					} else if(wordPos.startsWith("JJ")) {
						
					}
					
					if(!negate) {
						//String word = tokens.get(i).get(TextAnnotation.class);
						for(String negationToken: NEGATION_TOKENS) {
							if(negationToken.equals(word)) {
								negate = true;
								break;
							}
						}
					}
				}
			}
			dataset.add(new RVFDatum<String, String>(featureCounter, doc.getClassification()));
			String temp = featureCounter.toString();
			double rnd = Math.random()*100;
			
			if ((rnd > TestSplit) &&( Num_Test <=CorpusMax)){
			trainf.write(doc.getClassification() +"	" +featureCounter.toString()+"\n");
			Num_Train++;}
			else{
				testf.write(doc.getClassification() +"	" +featureCounter.toString()+"\n");
			Num_Test++;}
		}
		String resp = "Total items: " + (Num_Test+Num_Train) + "Total test items: " + Num_Test + "Total train items: " + Num_Train ;
		System.out.println(CorpusMax);
		System.out.println("Data files created");
		System.out.println("Total items: " + (Num_Test+Num_Train));
		System.out.println("Total test items: " + Num_Test);
		System.out.println("Total train items: " + Num_Train);
		return resp;
		}
	
	public void PrintResults(){
		
		System.out.println("Data files created");
		System.out.println("Total items: " + (Num_Test+Num_Train));
		System.out.println("Total test items: " + Num_Test);
		System.out.println("Total train items: " + Num_Train);
		}
	public String LineGenerator(String line){
	
		
		return line;
	}
}
	

